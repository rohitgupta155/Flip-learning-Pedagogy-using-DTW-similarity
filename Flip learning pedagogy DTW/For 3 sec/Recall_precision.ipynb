{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 1. 1. 0. 1. 0.]\n",
      " [1. 0. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 1. 1. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0. 1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 1. 0. 0. 1. 0. 1.]\n",
      " [0. 0. 1. 1. 0. 1. 0. 1. 0. 0.]]\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      " [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]]\n",
      "................ 1\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[0. 0. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
      "Confusion Matrix :\n",
      "[[0 0]\n",
      " [7 3]]\n",
      "Accuracy Score : 0.3\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         0\n",
      "         1.0       1.00      0.30      0.46        10\n",
      "\n",
      "    accuracy                           0.30        10\n",
      "   macro avg       0.50      0.15      0.23        10\n",
      "weighted avg       1.00      0.30      0.46        10\n",
      "\n",
      "................ 2\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      "Confusion Matrix :\n",
      "[[0 0]\n",
      " [8 2]]\n",
      "Accuracy Score : 0.2\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         0\n",
      "         1.0       1.00      0.20      0.33        10\n",
      "\n",
      "    accuracy                           0.20        10\n",
      "   macro avg       0.50      0.10      0.17        10\n",
      "weighted avg       1.00      0.20      0.33        10\n",
      "\n",
      "................ 3\n",
      "[1. 1. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      "Confusion Matrix :\n",
      "[[5 1]\n",
      " [2 2]]\n",
      "Accuracy Score : 0.7\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.83      0.77         6\n",
      "         1.0       0.67      0.50      0.57         4\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.69      0.67      0.67        10\n",
      "weighted avg       0.70      0.70      0.69        10\n",
      "\n",
      "................ 4\n",
      "[1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "[0. 0. 0. 0. 0. 1. 1. 0. 1. 0.]\n",
      "Confusion Matrix :\n",
      "[[1 0]\n",
      " [6 3]]\n",
      "Accuracy Score : 0.4\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.14      1.00      0.25         1\n",
      "         1.0       1.00      0.33      0.50         9\n",
      "\n",
      "    accuracy                           0.40        10\n",
      "   macro avg       0.57      0.67      0.38        10\n",
      "weighted avg       0.91      0.40      0.47        10\n",
      "\n",
      "................ 5\n",
      "[1. 1. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Confusion Matrix :\n",
      "[[5 0]\n",
      " [5 0]]\n",
      "Accuracy Score : 0.5\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      1.00      0.67         5\n",
      "         1.0       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.50        10\n",
      "   macro avg       0.25      0.50      0.33        10\n",
      "weighted avg       0.25      0.50      0.33        10\n",
      "\n",
      "................ 6\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 0. 0. 0. 1. 0. 0. 1. 1. 0.]\n",
      "Confusion Matrix :\n",
      "[[0 0]\n",
      " [6 4]]\n",
      "Accuracy Score : 0.4\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         0\n",
      "         1.0       1.00      0.40      0.57        10\n",
      "\n",
      "    accuracy                           0.40        10\n",
      "   macro avg       0.50      0.20      0.29        10\n",
      "weighted avg       1.00      0.40      0.57        10\n",
      "\n",
      "................ 7\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "Confusion Matrix :\n",
      "[[7 3]\n",
      " [0 0]]\n",
      "Accuracy Score : 0.7\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.70      0.82        10\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.50      0.35      0.41        10\n",
      "weighted avg       1.00      0.70      0.82        10\n",
      "\n",
      "................ 8\n",
      "[1. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      "[0. 0. 1. 1. 0. 1. 1. 0. 1. 0.]\n",
      "Confusion Matrix :\n",
      "[[3 2]\n",
      " [2 3]]\n",
      "Accuracy Score : 0.6\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.60      0.60         5\n",
      "         1.0       0.60      0.60      0.60         5\n",
      "\n",
      "    accuracy                           0.60        10\n",
      "   macro avg       0.60      0.60      0.60        10\n",
      "weighted avg       0.60      0.60      0.60        10\n",
      "\n",
      "................ 9\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[1. 0. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "Confusion Matrix :\n",
      "[[3 5]\n",
      " [2 0]]\n",
      "Accuracy Score : 0.3\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.38      0.46         8\n",
      "         1.0       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.30        10\n",
      "   macro avg       0.30      0.19      0.23        10\n",
      "weighted avg       0.48      0.30      0.37        10\n",
      "\n",
      "................ 10\n",
      "[1. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[1. 0. 0. 0. 1. 1. 1. 0. 1. 0.]\n",
      "Confusion Matrix :\n",
      "[[3 4]\n",
      " [2 1]]\n",
      "Accuracy Score : 0.4\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.43      0.50         7\n",
      "         1.0       0.20      0.33      0.25         3\n",
      "\n",
      "    accuracy                           0.40        10\n",
      "   macro avg       0.40      0.38      0.38        10\n",
      "weighted avg       0.48      0.40      0.42        10\n",
      "\n",
      "................ 11\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 1. 0. 0. 1. 0. 0. 1. 0.]\n",
      "Confusion Matrix :\n",
      "[[6 4]\n",
      " [0 0]]\n",
      "Accuracy Score : 0.6\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.60      0.75        10\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.60        10\n",
      "   macro avg       0.50      0.30      0.37        10\n",
      "weighted avg       1.00      0.60      0.75        10\n",
      "\n",
      "................ 12\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
      "Confusion Matrix :\n",
      "[[6 4]\n",
      " [0 0]]\n",
      "Accuracy Score : 0.6\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.60      0.75        10\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.60        10\n",
      "   macro avg       0.50      0.30      0.37        10\n",
      "weighted avg       1.00      0.60      0.75        10\n",
      "\n",
      "................ 13\n",
      "[1. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Confusion Matrix :\n",
      "[[5 2]\n",
      " [3 0]]\n",
      "Accuracy Score : 0.5\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.71      0.67         7\n",
      "         1.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.50        10\n",
      "   macro avg       0.31      0.36      0.33        10\n",
      "weighted avg       0.44      0.50      0.47        10\n",
      "\n",
      "................ 14\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      "Confusion Matrix :\n",
      "[[7 3]\n",
      " [0 0]]\n",
      "Accuracy Score : 0.7\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.70      0.82        10\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.50      0.35      0.41        10\n",
      "weighted avg       1.00      0.70      0.82        10\n",
      "\n",
      "................ 15\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      "Confusion Matrix :\n",
      "[[4 6]\n",
      " [0 0]]\n",
      "Accuracy Score : 0.4\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.40      0.57        10\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.40        10\n",
      "   macro avg       0.50      0.20      0.29        10\n",
      "weighted avg       1.00      0.40      0.57        10\n",
      "\n",
      "................ 16\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 1. 0. 1. 1. 1. 0. 0.]\n",
      "Confusion Matrix :\n",
      "[[4 5]\n",
      " [1 0]]\n",
      "Accuracy Score : 0.4\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.44      0.57         9\n",
      "         1.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.40        10\n",
      "   macro avg       0.40      0.22      0.29        10\n",
      "weighted avg       0.72      0.40      0.51        10\n",
      "\n",
      "................ 17\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Confusion Matrix :\n",
      "[[7 2]\n",
      " [1 0]]\n",
      "Accuracy Score : 0.7\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.78      0.82         9\n",
      "         1.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.44      0.39      0.41        10\n",
      "weighted avg       0.79      0.70      0.74        10\n",
      "\n",
      "................ 18\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
      "Confusion Matrix :\n",
      "[[7 3]\n",
      " [0 0]]\n",
      "Accuracy Score : 0.7\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.70      0.82        10\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.50      0.35      0.41        10\n",
      "weighted avg       1.00      0.70      0.82        10\n",
      "\n",
      "................ 19\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 1. 0. 1. 0. 0. 1. 0. 1.]\n",
      "Confusion Matrix :\n",
      "[[5 5]\n",
      " [0 0]]\n",
      "Accuracy Score : 0.5\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.50      0.67        10\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.50        10\n",
      "   macro avg       0.50      0.25      0.33        10\n",
      "weighted avg       1.00      0.50      0.67        10\n",
      "\n",
      "................ 20\n",
      "[0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]\n",
      "[0. 0. 1. 1. 0. 1. 0. 1. 0. 0.]\n",
      "Confusion Matrix :\n",
      "[[5 3]\n",
      " [1 1]]\n",
      "Accuracy Score : 0.6\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.62      0.71         8\n",
      "         1.0       0.25      0.50      0.33         2\n",
      "\n",
      "    accuracy                           0.60        10\n",
      "   macro avg       0.54      0.56      0.52        10\n",
      "weighted avg       0.72      0.60      0.64        10\n",
      "\n",
      "Average Accuracy: 0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\KIIT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Without cluster 1 second data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "#data = np.loadtxt('data.txt')\n",
    "data_rating =pd.read_csv(r\"C:\\Users\\KIIT\\Downloads\\Flip learning pedagogy DTW\\For 3 sec\\Labels_for_Beta_Alpha(0-1).csv\", header=None)\n",
    "data_rating=np.array(data_rating)\n",
    "print(data_rating)\n",
    "\n",
    "Ground_trute =  pd.read_csv(r\"C:\\Users\\KIIT\\Downloads\\Flip learning pedagogy DTW\\For 3 sec\\similarity.csv\", header=None)\n",
    "Ground_trute=np.array(Ground_trute)\n",
    "print(Ground_trute)\n",
    "a=0\n",
    "\n",
    "for i in range(0, len(Ground_trute)):\n",
    "    print(\"................\",(i+1))\n",
    "    actual = Ground_trute[i]\n",
    "    predicted = data_rating[i]\n",
    "    print(actual)\n",
    "    print(predicted)\n",
    "    results = confusion_matrix(actual, predicted) \n",
    "  \n",
    "    print ('Confusion Matrix :')\n",
    "    print(results) \n",
    "    accuracy=accuracy_score(actual, predicted) \n",
    "    a+=accuracy\n",
    "    print ('Accuracy Score :',accuracy)\n",
    "    print ('Report : ')\n",
    "    print (classification_report(actual, predicted)) \n",
    "print(\"Average Accuracy:\",a/20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 1. 1. 0. 1. 0.]\n",
      " [1. 0. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 1. 1. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0. 1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 1. 0. 0. 1. 0. 1.]\n",
      " [0. 0. 1. 1. 0. 1. 0. 1. 0. 0.]]\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      " [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]]\n",
      "Confusion Matrix:\n",
      " [[83 52]\n",
      " [46 19]]\n",
      "Accuracy: 0.510\n",
      "Precision: 0.456\n",
      "Recall: 0.454\n",
      "F1-Score: 0.454\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# assume actual and predicted are two numpy arrays of the same shape\n",
    "# actual = np.array([[1, 0], [1, 1], [0, 1], [0, 0], [1, 0]])\n",
    "# predicted = np.array([[1, 0], [0, 1], [0, 1], [0, 0], [1, 1]])\n",
    "predicted =pd.read_csv(r\"C:\\Users\\KIIT\\Downloads\\Flip learning pedagogy DTW\\For 3 sec\\Labels_for_Beta_Alpha(0-1).csv\", header=None)\n",
    "predicted=np.array(predicted)\n",
    "print(predicted)\n",
    "\n",
    "actual =  pd.read_csv(r\"C:\\Users\\KIIT\\Downloads\\Flip learning pedagogy DTW\\For 3 sec\\similarity.csv\", header=None)\n",
    "actual=np.array(actual)\n",
    "print(actual)\n",
    "# calculate confusion matrix\n",
    "cm = confusion_matrix(actual.ravel(), predicted.ravel())\n",
    "\n",
    "# calculate accuracy\n",
    "acc = accuracy_score(actual.ravel(), predicted.ravel())\n",
    "\n",
    "# calculate precision\n",
    "precision = precision_score(actual.ravel(), predicted.ravel(), average='macro')\n",
    "\n",
    "# calculate recall\n",
    "recall = recall_score(actual.ravel(), predicted.ravel(), average='macro')\n",
    "\n",
    "# calculate f1-score\n",
    "f1 = f1_score(actual.ravel(), predicted.ravel(), average='macro')\n",
    "\n",
    "# print results\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Accuracy: {:.3f}\".format(acc))\n",
    "print(\"Precision: {:.3f}\".format(precision))\n",
    "print(\"Recall: {:.3f}\".format(recall))\n",
    "print(\"F1-Score: {:.3f}\".format(f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
